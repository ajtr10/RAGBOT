# ğŸ¤– RAGBOT - PDF Q&A Chatbot

A Retrieval-Augmented Generation (RAG) chatbot that answers questions from your uploaded PDFs using vector search and LLMs.

![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

## âœ¨ Features

- ğŸ“„ **Multiple PDF Upload** - Ingest and index multiple documents
- ğŸ” **Semantic Search** - Find relevant context using vector similarity
- ğŸ¯ **Grounded Answers** - LLM responses constrained to document context
- ğŸ“Œ **Source Citations** - Track which PDFs answered your question
- ğŸ¨ **Clean UI** - Simple Streamlit interface for upload and chat
- â˜ï¸ **Production Ready** - Deployed on AWS EC2 with PM2

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit  â”‚â”€â”€â”€â”€â”€â–¶â”‚   FastAPI    â”‚â”€â”€â”€â”€â”€â–¶â”‚  Pinecone   â”‚
â”‚   (Client)  â”‚      â”‚   (Server)   â”‚      â”‚ (Vector DB) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚  Groq LLM    â”‚
                     â”‚ (llama-3.3)  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Workflow:**
1. Upload PDFs â†’ Parse & Chunk â†’ Embed (HuggingFace) â†’ Store (Pinecone)
2. Ask Question â†’ Retrieve Top-K Chunks â†’ Prompt LLM â†’ Return Answer + Sources

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|-----------|
| **Frontend** | Streamlit |
| **Backend** | FastAPI |
| **Vector DB** | Pinecone (serverless) |
| **Embeddings** | HuggingFace (`all-MiniLM-L12-v2`, 384-dim) |
| **LLM** | Groq (`llama-3.3-70b-versatile`) |
| **Framework** | LangChain (RetrievalQA) |
| **Deployment** | AWS EC2, PM2, Nginx |

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Pinecone API Key ([Get one here](https://www.pinecone.io/))
- Groq API Key ([Get one here](https://console.groq.com/))

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/ragbot.git
cd ragbot

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r server/requirements.txt
```

### Configuration

Create a `.env` file in the root directory:

```env
PINECONE_API_KEY=your_pinecone_key_here
GROQ_API_KEY=your_groq_key_here
PINECONE_NAMESPACE=resumes
```

### Run Locally

**Terminal 1 - Start Backend:**
```bash
uvicorn server.working_main:app --reload --port 8000
```

**Terminal 2 - Start Frontend:**
```bash
streamlit run client/app.py
```

Access the app at `http://localhost:8501`

## ğŸ“ Project Structure

```
ragbot/
â”œâ”€â”€ client/
â”‚   â”œâ”€â”€ app.py                    # Main Streamlit app
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ upload.py            # PDF uploader
â”‚   â”‚   â”œâ”€â”€ chatUI.py            # Chat interface
â”‚   â”‚   â””â”€â”€ history_download.py # Chat history
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ api.py               # API client
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ working_main.py          # FastAPI endpoints & RAG logic
â”‚   â”œâ”€â”€ logger.py                # Logging configuration
â”‚   â””â”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ uploaded_pdfs/               # PDF storage directory
â”œâ”€â”€ .env                         # Environment variables (not in repo)
â”œâ”€â”€ .env.example                 # Template for .env
â””â”€â”€ ecosystem.config.js          # PM2 configuration
```

## ğŸ”Œ API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/upload_pdfs/` | Upload and index PDFs |
| `POST` | `/ask/` | Ask a question |
| `GET` | `/sources` | List uploaded PDFs |
| `GET` | `/test` | Health check |

### Example Request

```bash
# Upload PDFs
curl -X POST "http://localhost:8000/upload_pdfs/" \
  -F "files=@document.pdf"

# Ask a question
curl -X POST "http://localhost:8000/ask/" \
  -H "Content-Type: application/json" \
  -d '{"question": "What is the main topic?", "source": "uploaded_pdfs/document.pdf"}'
```

## â˜ï¸ AWS Deployment

### Infrastructure

- **EC2 Instance:** Ubuntu 22.04 LTS (t3.medium recommended)
- **Process Manager:** PM2 for auto-restart and monitoring
- **Reverse Proxy:** Nginx (optional)
- **Security:** UFW firewall, SSH key authentication

### Deployment Steps

```bash
# 1. SSH into EC2
ssh -i your-key.pem ubuntu@your-ec2-ip

# 2. Clone repository
git clone https://github.com/yourusername/ragbot.git
cd ragbot

# 3. Set up environment
python3 -m venv venv
source venv/bin/activate
pip install -r server/requirements.txt

# 4. Configure .env
nano .env
# Add your API keys
chmod 600 .env

# 5. Install PM2
sudo npm install -g pm2

# 6. Start applications
pm2 start ecosystem.config.js
pm2 save
pm2 startup
```

Access at: `http://your-ec2-ip:8501`

## âš™ï¸ Configuration

### Retrieval Parameters

```python
# In server/working_main.py
search_kwargs = {
    "k": 10,                    # Top 10 chunks
    "score_threshold": 0.2      # Minimum similarity score
}
```

### LLM Settings

```python
llm = ChatGroq(
    model_name="llama-3.3-70b-versatile",
    temperature=0.0             # Deterministic output
)
```

### Pinecone Index

- **Name:** `rag-minillm-384`
- **Dimensions:** 384
- **Region:** us-east-1 (AWS serverless)

## ğŸ› Troubleshooting

### "I don't know" Responses

- Verify PDFs uploaded successfully: Check `uploaded_pdfs/` directory
- Ensure consistent `PINECONE_NAMESPACE` between upload and query
- Lower `score_threshold` (0.2 â†’ 0.15) for broader recall
- Check logs: `pm2 logs ragbot-server`

### Out of Memory (EC2)

- Upgrade from t3.small (2GB) to t3.medium (4GB)
- HuggingFace model requires ~120MB on first run
- Monitor: `free -h` and `htop`

### Connection Issues

```bash
# Check if apps are running
pm2 status

# Check ports
sudo netstat -tlnp | grep -E '8000|8501'

# Verify security group allows ports 8000, 8501 in AWS Console
```

## ğŸ“Š Performance

- **Embedding Model:** 384 dimensions, ~5MB model size
- **Retrieval Speed:** ~200ms for k=10 chunks
- **LLM Response:** ~2-4 seconds (Groq inference)
- **Memory Usage:** ~1.5GB baseline (server + client)

## ğŸ” Security Best Practices

- âœ… Store API keys in `.env` (never commit to Git)
- âœ… Use `chmod 600 .env` for file permissions
- âœ… Enable UFW firewall on EC2
- âœ… Use SSH key authentication (disable password auth)
- âœ… Keep dependencies updated: `pip install --upgrade -r server/requirements.txt`

## ğŸ“ Example Usage

1. **Upload PDFs:**
   - Click "Upload PDFs" in sidebar
   - Select one or more PDF files
   - Click "Upload to DB"

2. **Ask Questions:**
   - Type question in chat input: "What are the main findings?"
   - Get answer with source citations
   - Optionally filter by specific PDF

3. **View Sources:**
   - Sources listed below each answer
   - Format: `uploaded_pdfs/filename.pdf`

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Commit changes: `git commit -m 'Add feature'`
4. Push to branch: `git push origin feature-name`
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [LangChain](https://www.langchain.com/) for RAG framework
- [Pinecone](https://www.pinecone.io/) for vector database
- [Groq](https://groq.com/) for fast LLM inference
- [HuggingFace](https://huggingface.co/) for embedding models

## ğŸ“§ Contact

For questions or support, please open an issue or contact [your-email@example.com]

---

**Built with â¤ï¸ using RAG architecture**
